==============
About the project
==============

--------------
Problem Identification and Analysis
--------------

 Recently, generative language models (GPT, Bing, etc.) have been generating hallucinations.

 The source of the problem may be in the model's parameters or training method, but it's difficult to fix it immediately.

 So we saw a need for a tool that would allow users to verify the accuracy of model answers and provide information about the reason behind them.


--------------
Project Naming and Branding
--------------

 The project name is combined with “Honest”, the credibility of the text, and “LLM”, the Large Language Model.

 This name means to eliminate hallucinations and expect LLM to hear honest answers.

--------------
Mission statement
--------------
 We are starting the HonestLLM open-source project.

 The goal of the project is to develop technologies to determine the authenticity of the output of AI models and to prevent hallucinations. Users will be able to visually check the reliability of their answers using a fact-based database.

 Through this, we will create a safe and reliable artificial intelligence environment and a more certain and advanced future.

--------------
What Will It Do?
--------------

This project aims to allow users to check the accuracy of the output of a generative language model for correct answers and to provide a source of evidence.

 1. Build a database for a specific domain and utilize it as a basis for checking the accuracy of the answer.


 2. Calculate the similarity of the LLM model's answers to the evidence retrieved from the DB.


 3. Increase reliability by specifying the source of the grounds. 


--------------
Preferred Development Language(s)
--------------

* Python for LLM prompting.
* SQL for database queries.
